---
title: "logistic"
author: "STAT 6021"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
```



### Load In Data

```{r}
sleep <- read.csv("Sleep_Efficiency.csv")

# Remove ID, bedtime, and wakeup time
sleep <- sleep[, -c(1, 4, 5)]
```

### Cleaning 
```{r}
# Weighted sleep (Sleep.duration * Sleep.efficiency)
sleep <- sleep %>% 
  mutate(weighted.sleep = Sleep.duration * Sleep.efficiency)

sleep <- na.omit(sleep[, c("Sleep.duration","Sleep.efficiency", "Age", "Caffeine.consumption", "Alcohol.consumption", "Smoking.status", "Exercise.frequency")])

sleep$Enough.Sleep <- ifelse(
  sleep$Sleep.duration >= 7.5, 
  1, 
  0)

```


### Initial Logit with >= 7.5 Hours
```{r}
logit <- glm(
  Enough.Sleep ~ Age + Caffeine.consumption + 
    Alcohol.consumption + Smoking.status + Exercise.frequency, 
  sleep,
  family="binomial"
)
summary(logit)
```
It looks like we do not have many significant variables below the 0.1 threshold, only our intercept and alcohol consumption. What we can do is we can compare this with a full model forward selection and see which one this selects to select variables.

```{r}
# Null model (no predictors)
null_model <- glm(Enough.Sleep ~ 1, data = sleep, family = binomial)

# Full model (all predictors)
full_model <- glm(
  Enough.Sleep ~ Age + Caffeine.consumption + 
    Alcohol.consumption + Smoking.status + Exercise.frequency, 
  data = sleep,
  family = binomial
)

# Forward selection
forward_model <- step(null_model, 
                      scope = list(lower = null_model, upper = full_model), 
                      direction = "forward")

# Summary of the selected model
summary(forward_model)

```
Now, we have a significant intercept, and the p value for alcohol consumption is lower. This does not give us many variables to work with, so it looks like we should go back and explore more, maybe with interaction effects. Exercise frequency continues to perform poorly, so let's remove it and rerun the logistic, but accounting for all interaction terms.

```{r}
logit <- glm(
  Enough.Sleep ~ (Age + Caffeine.consumption + Alcohol.consumption + Smoking.status)^2, 
  data = sleep,
  family = binomial
)

summary(logit)

```
It looks like Age and Alcohol consumption have some interaction and Alcohol consumption and smoking too, though not at a 0.1 threshold. Let's include these variables in our model and see what happens.

```{r}
logit <- glm(
  Enough.Sleep ~ Age*(Alcohol.consumption + Smoking.status), 
  data = sleep,
  family = binomial
)

summary(logit)
```
Interesting. It also looks like alcohol on its own is very important on its own, given how low the p value is for the beta coefficient estimate. Let's run a forward selection with these variables to see what the best performing model is, since it looks like we have some more significant items.

```{r}
null_model <- glm(Enough.Sleep ~ 1, data = sleep, family = binomial)

full_model <- glm(
  Enough.Sleep ~ Age*(Alcohol.consumption + Smoking.status), 
  data = sleep,
  family = binomial
)

forward_model <- step(null_model, 
                      scope = list(lower = null_model, upper = full_model), 
                      direction = "forward")

# Summary of the selected model
summary(forward_model)
```

Again, the most important variable is going to be using alcohol consumption, and it still falls below above the 0.05 p value, but the intercept and alcohol are both still significant at the 0.1 threshold, indicating that the model may be somewhat useful. This is the model with the lowest AIC, but let us compare this one and the one above (which had multiple beta coefficient estimates below p = 0.05 rather than one) for predictions.

### Example Prediction

Instead of measuring by the AIC for the best model, let's compare the results from the residuals for our data.
```{r}
new.data <- sleep %>% 
  select(
    Age,
    Alcohol.consumption,
    Smoking.status
  )

new.data.forward <- data.frame(Alcohol.consumption=sleep$Alcohol.consumption)

pred.logit <- ifelse(
  predict(logit, newdata = new.data, type = "response") > 0.5,1,0)

pred.forward <- ifelse(
  predict(forward_model, newdata = new.data.forward, type = "response") > 0.5,1,0)
```

```{r}
library(caret)

confusion_matrix_logit <- confusionMatrix(
  as.factor(pred.logit), 
  as.factor(sleep$Enough.Sleep)
)

print(confusion_matrix_logit)
```

```{r}
confusion_matrix_forward <- confusionMatrix(
  as.factor(pred.forward), 
  as.factor(sleep$Enough.Sleep)
)

print(confusion_matrix_forward)
```

```{r}
library(ggplot2)

predicted_probabilities <- predict(logit, newdata = new.data, type = "response")

# Plot histogram
hist(predicted_probabilities, breaks = 20, col = "blue", 
     main = "Histogram of Predicted Probabilities for Logit",
     xlab = "Predicted Probability", ylab = "Frequency",
     border = "black")
```

```{r}
predicted_probabilities <- predict(
  forward_model, newdata = new.data.forward, type = "response")

# Plot histogram
hist(predicted_probabilities, breaks = 20, col = "blue", 
     main = "Histogram of Predicted Probabilities from Forward Selection Model",
     xlab = "Predicted Probability", ylab = "Frequency",
     border = "black")
```

Both models result in similarly low scores for accuracy, around 50%. Still, both models are above that mark, and the one that only uses alcohol has an accuracy around 59%, which is also higher than the model from logistic regression with multiple variables instead of only alcohol.

This above histogram plot starts on the right side for 0 drinks, and goes towards the left for each additional drink.


### Example prediction

We can see the histogram from above to see what happens with the number of drinks and likeliness of not enough sleep. Our beta coefficient is -0.11702. That means for each additional drink, the odds of getting enough sleep decrease by 11.04%.

```{r}
(1 - exp(-0.11702) ) * 100
```
Suppose we are at a party and we are thinking how many drinks we can have in order to get enough sleep. Let's plot our model and see the number of drinks we can have to still be over 50% predicted probability.

```{r}
sapply(seq(0, 5), function(x) {
  prob <- exp(0.39630 + -0.11702 * x) / (1 + exp(0.39630 + -0.11702 * x))
  cat(paste0("Drink ", x, ": ", round(prob,2), "\n"))
});
```
It looks like at drink 4, we are no longer predicting enough sleep.


